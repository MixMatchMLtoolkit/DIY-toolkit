<?xml version="1.0" encoding="UTF-8"?>
<combinations xmlns:xlink="http://www.w3.org/1999/xlink">
  <images>
  <im value='labeled'>assets/photos/ld.png</im>
  <im value='unlabeled'>assets/photos/ud.png</im>
  <im value='supervised'>assets/photos/sl.png</im>
  <im value='unsupervised'>assets/photos/us.png</im>
  <im value='output'>assets/photos/icons_v2-11.png</im>
  <im value='output_l'>assets/photos/output_l.png</im>
  <im value='combi'>assets/photos/icons_combi.svg</im>
  <im value='humanintheloop'>assets/photos/hitl.png</im>
</images>
  <combi>
    <name>Predictions based on images</name>
    <datatype>Image</datatype>
    <ability>Foresee</ability>
    <datatoken>98</datatoken>
    <abilitytoken>110</abilitytoken>
    <description>Images could also be used as input for making predictions about for example next actions, states e.g. emotional states (overlap with categorizing images, in this case will it predict your future emotion) and values (on e.g. a continuous scale). For this, the model will learn to recognize certain features in the images and link these with the provided ground truth.  </description>
    <output> <![CDATA[ Label 5 (82%), Label 3 (18%) <br/> OR a predicted value (e.g. 23)]]></output>
    <outputtoken>text</outputtoken>
    <techterm>Classification, regression</techterm>
    <examples>
      <ex>
        <exname>LikelyAI</exname>
        <exdescription> LikelyAI will use your images as input and predicts how popular they will be when you post it on Instagram and choose the best. In this case it is using regression as it shows a percentage score of expected popularity for each image. It uses already existing images with known popularity, and the machine learning model will try to find patterns in these.</exdescription>
        <eximage>assets/photos/likelyai.png</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://www.likelyai.com/"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://medium.com/analytics-vidhya/fastai-image-regression-age-prediction-based-on-image-68294d34f2ed"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Image recognition</name>
    <datatype>Image</datatype>
    <ability>Categorize</ability>
    <datatoken>98</datatoken>
    <abilitytoken>109</abilitytoken>
    <description>Images are easy to recognize for people, but can be very hard for computers as there will always be slight variations between photos of the same class, e.g. the angle or the lighting.
You  might for example want to recognize different animals in a photo or handwritten numbers. However not all cats look alike and everyone's handwritten 2 looks different. ML models for this ability will learn to recognize what is shown in the images. Some models can recognize only one object in each image and others can detect multiple objects, as well as their location. In the case of the latter, the images will not only need a label but also a bounding box around the different objects in the image with a label for what is in each bounding box.</description>
    <output> <![CDATA[ Label 5 (60%) <br/> Label 3 (23%) <br/> Label 2 (17%)]]></output>
    <outputtoken>text</outputtoken>
    <techterm>Image classification</techterm>
    <examples>
      <ex >
        <exname>Google Lens</exname>
        <exdescription> Google Lens has many different functionalities and one of them is recognizing what is in the image. It will show you the class to which your image belongs. For example, recognize the type of flower. However, it can only recognize objects on which it has been trained, so it might not be able to recognize it.</exdescription>
        <eximage>assets/photos/googlelens.jpg</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://techengage.com/google-lens-now-detects-billion-items/">Link to example</exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://www.tensorflow.org/tutorials/images/classification">Link to DIY</diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Image identification</name>
    <datatype>Image</datatype>
    <ability>Identify</ability>
    <datatoken>98</datatoken>
    <abilitytoken>111</abilitytoken>
    <description>Image identification is a specific subset of image recognition. Image recognition (categorize) places an image in one of the possible categories, but these categories are general "buckets" and do not identify individuals or individual objects. If you do want to be able to specifically identify an individual (e.g. to attach a name to an image) or an individual object (e.g. to recognize the name of a building), you can use image identification.</description>
    <output> <![CDATA[ Label 5 (60%) <br/> Label 3 (23%) <br/> Label 2 (17%)]]></output>
    <outputtoken>text</outputtoken>
    <techterm>Image classification</techterm>
    <examples>
      <ex >
        <exname>Google Lens Landmark</exname>
        <exdescription>Google Lens has many different functionalities, and one of them is recognizing specific landmarks in images. However, it can only recognize objects on which it has been trained, so it might not be able to recognize it.</exdescription>
        <eximage>assets/photos/googlelens.jpg</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://techengage.com/google-lens-now-detects-billion-items/">Link to example</exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://www.tensorflow.org/tutorials/images/classification">Link to DIY</diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Image clustering</name>
    <datatype>Image</datatype>
    <ability>Cluster</ability>
    <datatoken>98</datatoken>
    <abilitytoken>112</abilitytoken>
    <description>When the images are not labeled, but you still want to find images that might belong to the same category or you might want to find similar images for other purposes, you can use image clustering. The model will look for similarities between the images and group those together. It could for example cluster based on color or on the object visible in the image. However, since it is unsupervised learning, it will not be able to give labels to the clusters. </description>
    <output> <![CDATA[ Coordinates of the centroids <br> Which points belong to which cluster]]></output>
    <outputtoken>table</outputtoken>
    <techterm>Image clustering</techterm>
    <examples>
      <ex>
        <exname>Google Photos</exname>
        <exdescription>Google Photos will cluster your photos based on metadata it collected (time, location) but it will also try to cluster your images and place similar pictures together. At the end, it will also use categorization to describe what is in the cluster.</exdescription>
        <eximage>assets/photos/googlephotos.jpg</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://photos.google.com/explore"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://towardsdatascience.com/how-to-cluster-images-based-on-visual-similarity-cd6e7209fe34"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Image recommendation</name>
    <datatype>Image</datatype>
    <ability>Recommend</ability>
    <datatoken>98</datatoken>
    <abilitytoken>116</abilitytoken>
    <description>Image recommendation can use two main approaches. The first approach is content-based filtering, in this approach the machine learning model will look at what images are similar to the one you liked in the past and recommend similar images to you. The second approach is collaborative filtering, and in this case the machine learning model will look at what images similar users liked and recommend those images (user-user) or look at which images are also often liked when image X is liked by users and recommend those images (item-item).</description>
    <output> <![CDATA[ recommendation 1 <br> recommendation 2 <br> recommendation 3]]></output>
    <techterm>Recommender system images, collaborative filtering images, content based filtering images</techterm>
    <examples>
      <ex>
        <exname>Pinterest</exname>
        <exdescription>Pinterest is build around the ability of recommending images. On Pinterest you can "pin" images and based on what you have "pinned" and on what you  have searched, it will recommend other images that you probably like as well. For this is it will use a combination of the different recommender systems, as it will show you both more images that share the same content but also images that other users liked.</exdescription>
        <eximage>assets/photos/pinterest.png</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://pinterest.com/"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://medium.com/geekculture/building-a-visual-similarity-based-recommendation-system-using-python-872a5bea568e"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Image generation</name>
    <datatype>Image</datatype>
    <ability>Generate</ability>
    <datatoken>98</datatoken>
    <abilitytoken>115</abilitytoken>
    <description>The ability to generate new images can be used to generate new images based on already existing images, for example when you want to augment your dataset. It can also finish your sketches based on what it thinks you are drawing or generate images based on a text prompt, which might help when generating new (design) ideas.</description>
    <capabilities>
      <c1 value="Can create new content from scratch"></c1>
      <c2 value="Can augment your dataset so you have more data to train with"></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value="The model might only create one small subset of the data and fails to generalize"></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <output> <![CDATA[New image]]></output>
    <techterm>Generative adversarial network, image generation</techterm>
    <examples>
      <ex>
        <exname>Dall-e</exname>
        <exdescription>DALL-E uses the text captions that users can input to generate new images that visualize what was written in the text. It will generate different images for the same input so it can help for example with ideating or when you want to visualize something of which you do not have a photo.</exdescription>
        <eximage>assets/photos/dall-e.JPG</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://openai.com/blog/dall-e/"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://towardsdatascience.com/image-generation-in-10-minutes-with-generative-adversarial-networks-c2afc56bfa3b"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Distinguishing image</name>
    <datatype>Image</datatype>
    <ability>Distinguish</ability>
    <datatoken>98</datatoken>
    <abilitytoken>114</abilitytoken>
    <description>Based on a large set of (unlabeled) images, is it possible to find some general trends or an average. When you, or the model, have found this general trend, it is also possible to notice some images that are far from this general trend or average. These images can be outliers or anomalies. An alternative to unsupervised distinguishing is to use supervised data, where the data is labeled as either being an outlier or not. In this case, the model will learn to predict if it is an outlier or not.</description>
    <output> <![CDATA[ Possible outlier 1 <br> Possible outlier 2]]></output>
    <techterm>Anomaly detection</techterm>
    <examples exist='no'>
      <ex>
        <exname></exname>
        <exdescription></exdescription>
        <eximage>assets/photos/</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href=""></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href=""></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Image descriptions</name>
    <datatype>Image</datatype>
    <ability>Communicate</ability>
    <datatoken>98</datatoken>
    <abilitytoken>113</abilitytoken>
    <description>To be able to communicate what is visible in an image, a model will look at the content shown in the image and couple this to words or sentences. You can think of it as first recognizing the objects in the image (image categorization) and next crafting understandable sentences by combining all the labels of the recognized objects in the image. As an extra step, this text can also be converted to speech.</description>
    <output> <![CDATA[Text]]></output>
    <techterm> Image caption generation, NLP, image to text/speech</techterm>
    <examples>
      <ex>
        <exname>Alternative text</exname>
        <exdescription>Describes the content of an image placed in Word or another Microsoft application</exdescription>
        <eximage>assets/photos/alternativetext.jpg</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://blogs.microsoft.com/ai/azure-image-captioning/ "></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://medium.com/swlh/automatic-image-captioning-using-deep-learning-5e899c127387"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Image understanding</name>
    <datatype>Image</datatype>
    <ability>Understand</ability>
    <datatoken>98</datatoken>
    <abilitytoken>122</abilitytoken>
    <description>To understand what is visible in an image, you can train an image categorization or identification model. This will learn to label objects or elements in an image and use this to come to some sense of 'understanding'. However, it can never really understand it, as it will for example not be able to tell the cause of something. Moreover, it will only recognize on what it has been trained.</description>
    <output> <![CDATA[ Label 2 (67%) <br> Label 7 (33%)]]></output>
    <techterm> Image classification</techterm>
    <examples>
      <ex>
        <exname>Innereye</exname>
        <exdescription> InnerEye is a research project from Microsoft Research Cambridge with the aim to create automatic, quantitative analysis of three-dimensional medical images possible. For this, it uses techniques such as image categorization to recognize what is shown in the image.</exdescription>
        <eximage>assets/photos/innereye.jpg</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://www.microsoft.com/en-us/research/project/medical-image-analysis/"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://cainvas.ai-tech.systems/use-cases/breast-cancer-detection-app/"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Image translation</name>
    <datatype>Image</datatype>
    <ability>Translation</ability>
    <datatoken>98</datatoken>
    <abilitytoken>117</abilitytoken>
    <description>Image translation can be considered in two ways: (i) translating an image into text, this option can be found under image understanding, or (ii) as transfer the style from one image to another image. For the latter, you can use e.g. famous paintings as reference style and apply this style to your image. While it uses some models from Machine Learning it is not really ML, but can be seen as a side effect or a different way  of using the models. </description>
    <output> <![CDATA[A restyled image]]></output>
    <techterm>Neural style transfer</techterm>
    <examples>
      <ex>
        <exname>Deepart</exname>
        <exdescription> Based on the style of e.g. a famous painting, your uploaded image will be transformed to this style.  The models are trained on the reference images and when you upload your own image, the model will use what is has learned about the selected reference image and apply that style to your picture.</exdescription>
        <eximage>assets/photos/deepart.jpg</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://deepart.io/"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://www.tensorflow.org/tutorials/generative/style_transfer"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Text prediction</name>
    <datatype>Text</datatype>
    <ability>Foresee</ability>
    <datatoken>100</datatoken>
    <abilitytoken>110</abilitytoken>
    <description>Text prediction captures two different concepts. On the one hand, you can use text as input and have for a numerical value associated with it. For example, to predict the readability of a text.
The other concept is that you are predicting the text you are likely to type, these could be the next words or letters but also possible responses to an email.
For both concepts, the text will need to be preprocessed to make it understandable for machine learning models.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <output> <![CDATA[ Possible next word 1 (92 %) <br> Possible next word 2 (8%) <br> OR a predicted value (e.g. 23)]]></output>
    <techterm>Autocomplete, predictive text, NLP</techterm>
    <examples>
      <ex>
        <exname>Autocomplete Google</exname>
        <exdescription>When using the Google search engine, it will predict what you are probably searching and suggest several options for completing your search query. To do this, it will use your browsing history and data from all its users to predict what the most probable search query will be.</exdescription>
        <eximage>assets/photos/autocomplete.png</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://www.google.com/"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://medium.com/analytics-vidhya/build-a-simple-autocomplete-model-with-your-own-google-search-history-ead26b3b6bd4"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Text classification</name>
    <datatype>Text</datatype>
    <ability>Categorize</ability>
    <datatoken>100</datatoken>
    <abilitytoken>109</abilitytoken>
    <description>To match text with certain predefined categories, you need to have a model that is trained on text fragments with a label. For example, movie reviews are labeled as positive or negative or emails can be labeled as spam/ no spam.To be able to recognize the texts, they need to be pre-processed to make them understandable for machine learning models.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>Categories should be predefined, does not understand the text</limitations>
    <output> <![CDATA[ Label 5 (60%) <br/> Label 3 (23%) <br/> Label 2 (17%)]]></output>
    <techterm>Text classification, NLP</techterm>
    <examples>
      <ex>
        <exname>Sentiment analysis</exname>
        <exdescription>This tool allows you to analyse the sentiment of a fragment of text and will predict if it is positive or negative, and also shows how confident is that it has predicted it correctly. You could use this information to monitor reviews about your product on for example social media.</exdescription>
        <eximage>assets/photos/sentimentanalysis.JPG</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://monkeylearn.com/sentiment-analysis-online/">Link to example</exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://www.tensorflow.org/text/tutorials/text_classification_rnn">Link to DIY</diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Text identification</name>
    <datatype>Text</datatype>
    <ability>Identify</ability>
    <datatoken>100</datatoken>
    <abilitytoken>111</abilitytoken>
    <description>Text data might also be used to categorize the identity of a specific individual/item from a trait, given that the text data contains enough information to make distinctions between individuals. For example, the writing style might be linked to a specific writer. However, compared to e.g. facial recognition or fingerprints, text data is less likely to be unique to a person.</description>
    <output> <![CDATA[ Label 5 (60%) <br/> Label 3 (23%) <br/> Label 2 (17%)]]></output>
    <techterm>Text classification</techterm>
    <examples exist='no'>
      <ex>
        <exname></exname>
        <exdescription></exdescription>
        <eximage>assets/photos/</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href=""></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href=""></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Text clustering</name>
    <datatype>Text</datatype>
    <ability>Cluster</ability>
    <datatoken>100</datatoken>
    <abilitytoken>112</abilitytoken>
    <description>When no labels are present, texts can be clustered based on their similarities. For example, you might want to cluster them on writing style, the language in which it is written or the topics they are covering. In order to find these similarities between texts, you will first need to pre-process the data to make it understandable for the machine learning models.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <output> <![CDATA[ Coordinates of the centroids <br> Which words/texts belong to which cluster]]></output>
    <techterm>Text clustering, NLP</techterm>
    <examples>
      <ex>
        <exname>Google News</exname>
        <exdescription>Google News clusters articles from different sources that cover the same topic. For this, it will look at similarities between the news articles and group them together. At the end, the extra step of categorization is done to describe what is in these clusters.</exdescription>
        <eximage>assets/photos/googlenews.JPG</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://news.google.com/"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://towardsdatascience.com/making-sense-of-text-clustering-ca649c190b20"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Text recommendation</name>
    <datatype>Text</datatype>
    <ability>Recommend</ability>
    <datatoken>100</datatoken>
    <abilitytoken>116</abilitytoken>
    <description>Text recommendation can be done by using techniques to get an idea of e.g. the topics that are being addressed in the text (text understanding) and next recommend texts that have a similar topic to the one you liked or just read (content-based filtering). Another approach is not looking at what the text is discussing, but looking at what similar users read and recommending those texts to you (collaborative filtering).</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <output> <![CDATA[ recommendation 1 <br> recommendation 2 <br> recommendation 3]]></output>
    <techterm>Recommender system text, text collaborative filtering, text content based filtering, NLP</techterm>
    <examples>
      <ex>
        <exname>LinkedIn</exname>
        <exdescription>LinkedIn offers job recommendations based on your job searches, job alerts, profile, and activity on LinkedIn. This data is textual data and will be used to recommend jobs that are matching with what you have searched for, what you have liked or what you have written on your profile. For this, it will also need to analyse the job description to see what is being asked.</exdescription>
        <eximage>assets/photos/linkedin.png</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://www.linkedin.com/help/linkedin/answer/11783/job-recommendations-overview?lang=en"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://medium.com/@armandj.olivares/building-nlp-content-based-recommender-systems-b104a709c042"></diylink>
      </ex>
    </examples>
  </combi>

  <combi>
    <name>Text generation</name>
    <datatype>Text</datatype>
    <ability>Generate</ability>
    <datatoken>100</datatoken>
    <abilitytoken>115</abilitytoken>
    <description>Text generation can be used to generate completely new texts from scratch (based on a fewkey words), to complete texts or write it in a specific style (e.g. Shakespearean language). In order to have properly constructed sentences, this ML capability works together with other  capabilities: text understanding and communication, to write understandable texts.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <output> <![CDATA[New text]]></output>
    <techterm>Text generation, generative adversarial network (GAN), NLP</techterm>
    <examples>
      <ex>
        <exname>Plot generator</exname>
        <exdescription>This plot generator allows you to generate different types of texts such as fairy tales, movie scripts or headlines based on a few parameters that you need to set beforehand.</exdescription>
        <eximage>assets/photos/plotgenerator.JPG</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://www.plot-generator.org.uk/">"</exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://towardsdatascience.com/the-making-of-an-ai-storyteller-c3b8d5a983f5"></diylink>
      </ex>
    </examples>
  </combi>

  <combi>
    <name>Distinguish text</name>
    <datatype>Text</datatype>
    <ability>Distinguish</ability>
    <datatoken>100</datatoken>
    <abilitytoken>114</abilitytoken>
    <description>It is possible to distinguish texts that are abnormal, e.g. do not fit the general trend or are far from the mean. To do this, you often have to first transform the text into some other type of data: e.g. counting how often each word occurs and using these frequencies. Next you can compare instances and see if they are very similar or if they are possible outliers. An alternative to unsupervised distinguishing is to use supervised data, where the data is labeled as either being an outlier or not. In this case, the model will learn to predict if it is an outlier or not.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <output> <![CDATA[ Possible outlier 1 <br> Possible outlier 2]]></output>
    <techterm>Anomaly detection, NLP</techterm>
    <examples>
      <ex>
        <exname>Unique movies</exname>
        <exdescription>While anomalies might sound negative, it is also possible to use it to identify unique cases. This model will model the plot of movies into a vector space, where movies with the same plot will be close together. When you know this, you can also look at the movies who are the farthest from all the other movies, and these are the most unique movie plots.</exdescription>
        <eximage>assets/photos/uniquemovies.png</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://smartclick.ai/articles/how-artificial-intelligence-is-used-in-the-film-industry/">"</exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://medium.datadriveninvestor.com/unsupervised-outlier-detection-in-text-corpus-using-deep-learning-41d4284a04c8"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Text communication</name>
    <datatype>Text</datatype>
    <ability>Communicate</ability>
    <datatoken>100</datatoken>
    <abilitytoken>113</abilitytoken>
    <description>Quite some of our communication happens using text, e.g. emails, search queries, chats, etc. In order for an ML model to communicate via text, it often first needs to understand what is being said (text understanding) and next generate an appropriate response. This could be a likely reply to an email, an answer to a question or daily conversations.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <output> <![CDATA[Text]]></output>
    <techterm>NLP</techterm>
    <examples>
      <ex>
        <exname>Duolingo chatbot</exname>
        <exdescription>The chatbot from Duolingo helps you practice your conversation skills in French, German and Spanish. You could start a chat and the chatbot would recognize what you are saying and would respond to that.</exdescription>
        <eximage>assets/photos/duolingo.png</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://www.tristatetechnology.com/blog/best-language-learning-chatbot-apps/">"</exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://towardsdatascience.com/how-to-create-a-chatbot-with-python-deep-learning-in-less-than-an-hour-56a063bdfc44"></diylink>
      </ex>
    </examples>
  </combi>

  <combi>
    <name>Text understanding</name>
    <datatype>Text</datatype>
    <ability>Understand</ability>
    <datatoken>100</datatoken>
    <abilitytoken>122</abilitytoken>
    <description>For some interactions, it is needed that what you are typing is not only written down, but also that it is understood. For example, when typing an email it is not always necessary that the program knows what you are writing or reading, but if it does, it can e.g. suggest possible responses or remind you that you might have forgotten the attachment. This is still quite a superficial understanding, but when using chatbots, the program really should understand what you are typing as it will need to create a fitting response.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <output> <![CDATA[Text]]></output>
    <techterm>NLP, text classification</techterm>
    <examples>
      <ex>
        <exname>Code oracle</exname>
        <exdescription>Code is one very specific form of text and can be quite hard to understand, especially when using an unfamiliar language. Code Oracle allows you to highlight a snippet of your code to see in "normal" language what it is doing.</exdescription>
        <eximage>assets/photos/codexplainer.jpg</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://gpt3demo.com/apps/repl-it">"</exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://medium.com/analytics-vidhya/ai-generates-code-using-python-and-openais-gpt-3-2ddc95047cba"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Text translation</name>
    <datatype>Text</datatype>
    <ability>Translation</ability>
    <datatoken>100</datatoken>
    <abilitytoken>117</abilitytoken>
    <description>Text can be translated into other languages. To have a good translation is not so simple since words can have multiple meanings, the grammar should be correct and the word order might be different: making word for word translation a bad solution. For example, in French the adjective is in general placed after the noun, while in English it is placed before.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <output> <![CDATA[Translated text]]></output>
    <techterm>NLP, machine translation</techterm>
    <examples>
      <ex>
        <exname>Google Translate</exname>
        <exdescription>Google Translate recognizes the input text and predicts which language it is, or you can select the input language manually. Next it will translate the whole text to the output language and also give some possible alternatives, e.g. synonyms.</exdescription>
        <eximage>assets/photos/googletranslate.png</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://translate.google.com/">"</exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://medium.com/@ageitgey/build-your-own-google-translate-quality-machine-translation-system-d7dc274bd476"></diylink>
      </ex>
    </examples>
  </combi>

  <combi>
    <name>Time based predictions</name>
    <datatype>Time series</datatype>
    <ability>Foresee</ability>
    <datatoken>101</datatoken>
    <abilitytoken>110</abilitytoken>
    <description>Time series are suited for making predictions since one of the measurements (attributes) of time series data, is the time. This
makes it possible to predict what could happen by making e.g. a regression plot with the time on the X axis. Then you can either predict a continuous range of numbers or have discrete outcomes (only a certain number of categories or classes). For example, predicting the temperature tomorrow (continuous: scale of real numbers), or if it is going to rain in the upcoming hour (discrete: yes/no).</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <output> <![CDATA[ Possible next value 1 (88 %) <br> Possible next value 2 (12%) <br> OR a predicted value (e.g. 23)]]></output>
    <techterm>Forecasting, time series regression</techterm>
    <examples>
      <ex>
        <exname>Stock market</exname>
        <exdescription>While perfect stock market predictions are not (yet) possible, it is possible to recognize some general trends based on time series data. For the stock market, it is important to take the time component into account, which is why time series data is used. So based on the historic data, the model will try to find patterns and use these to predict the future.</exdescription>
        <eximage>assets/photos/stockmarket.JPG</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://www.wallstreetzen.com/stock-screener/stock-forecast"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://data-flair.training/blogs/stock-price-prediction-machine-learning-project-in-python/"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Time series categorization</name>
    <datatype>Time series</datatype>
    <ability>Categorize</ability>
    <datatoken>101</datatoken>
    <abilitytoken>109</abilitytoken>
    <description>For time series categorization (or classification) you will need to have labeled time series data. Often time series data will be first pre-processed and afterwards labeled, e.g. taking the means of every second of sensor data and assign that to one category. Next, the model will learn how to match the time series with the categories (or classes).</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <output> <![CDATA[ Label 5 (60%) <br/> Label 3 (23%) <br/> Label 2 (17%)]]></output>
    <techterm>Time series classification</techterm>
    <examples>
      <ex>
        <exname>Acitivity recognition</exname>
        <exdescription>To recognize activity, data from sensors such as accelerometers (measuring the acceleration in the X, Y and Z axis), ECG (heartbeat) and GPS (your location and based on this how fast you are moving) can be used. These data are time series, since one of the measurements (or features) is the time of measuring the other features. Based on the sensor data, classes can be assigned to each fragment of data and the model can learn to recognize what data pattern belongs to cycling or walking, standing still, etc.</exdescription>
        <eximage>assets/photos/activityrecognition.JPG</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://tinyurl.com/ytms74s9"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://machinelearningmastery.com/how-to-load-and-explore-a-standard-human-activity-recognition-problem/"></diylink>
      </ex>
    </examples>
  </combi>
  <combi >
    <name>Time series identification</name>
    <datatype>Time series</datatype>
    <ability>Identify</ability>
    <datatoken>101</datatoken>
    <abilitytoken>111</abilitytoken>
    <description>Time series are data sets which have the accompanying timestamp for each measurement. These can be recorded in a tabular format and/or visualized in a graph with the time on the x-axis. When segments of these time series are labeled with the corresponding individual/ item, it might be possible to identify a specific individual/ item. However, the data should contain enough information to learn this distinction.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <output> <![CDATA[ Label 5 (60%) <br/> Label 3 (23%) <br/> Label 2 (17%)]]></output>
    <techterm>Time series classification</techterm>
    <examples exist='no'>
      <ex>
        <exname></exname>
        <exdescription></exdescription>
        <eximage>assets/photos/</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href=""></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href=""></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Time series clustering</name>
    <datatype>Time series</datatype>
    <ability>Cluster</ability>
    <datatoken>101</datatoken>
    <abilitytoken>112</abilitytoken>
    <description>Time series clustering resembles time series categorization somewhat, but the difference is that with clustering, you only look at which data points seem similar and group these together. So you will start with time series data, and these often need to be pre-processed. Next these data are analyzed for patterns and afterwards the different data fragments will be assigned to one of the clusters.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
        <output> <![CDATA[ Coordinates of the centroids <br> Which points belong to which cluster]]></output>
    <techterm>Time series clustering</techterm>
    <examples>
      <ex>
        <exname>Clustering earthquake signals</exname>
        <exdescription>Monitoring seismic data is very labor-intensive and there is more data than can be currently analyzed in a supervised manner. Therefore, clustering can be used to find patterns in these data and use these clusters to for example forecast earthquakes.</exdescription>
        <eximage>assets/photos/earthquake.JPG</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://www.nature.com/articles/s41467-020-17841-x"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://www.kaggle.com/izzettunc/introduction-to-time-series-clustering"></diylink>
      </ex>
    </examples>
  </combi>
    <combi>
    <name>Time series recommendation</name>
    <datatype>Time series</datatype>
    <ability>Recommend</ability>
    <datatoken>101</datatoken>
    <abilitytoken>116</abilitytoken>
    <description>Recommendations are often about what to recommend to someone based on, e.g. products they bought before, or music you regularly listen. But time series can also be used in recommendation as this will take into account when you are doing all these things. Therefore, it can also take that into account when making recommendations. In this case, an extra attribute (column) is added to your dataset with the time, while the other columns will still contain e.g. the products you bought or which pages you visited.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <output> <![CDATA[ Recommendation 1 <br> Recommendation 2 <br> Recommendation 3 ]]></output>
    <techterm>Time series recommender systems, latent dirichlet allocation time series</techterm>
    <examples>
      <ex>
        <exname>Netflix</exname>
        <exdescription>Netflix records what you have watched, which movies you liked etc., and also when you watched certain shows. By recording the timestamps of the actions, the data become time series data, and the recommendations can be time dependent. So if you are always watching a show in the morning, Netflix will only recommend it in the morning and recommend different shows in the evening.</exdescription>
        <eximage>assets/photos/netflix.jpg</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://help.netflix.com/en/node/100639"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://towardsdatascience.com/create-a-recommendation-system-based-on-time-series-data-using-latent-dirichlet-allocation-2aa141b99e19"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Synthetic time series</name>
    <datatype>Time series</datatype>
    <ability>Generate</ability>
    <datatoken>101</datatoken>
    <abilitytoken>115</abilitytoken>
    <description>Time series are data sets which have the accompanying timestamp for each measurement. These can be recorded in a tabular format and/or visualized in a graph with the time on the x-axis. Based on existing data you can generate more time series data which can afterwards be used for e.g. training a better machine learning model with more data.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
        <output> <![CDATA[New time series]]></output>
    <techterm>Generative adversarial networks (GAN), synthetic time series</techterm>
    <examples>
      <ex>
        <exname>Synthetic time series data</exname>
        <exdescription>Creating extra time series data based on some starting data. More data can help to train a better model.</exdescription>
        <eximage>assets/photos/synthetictimeseries.jpg</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://www.osti.gov/servlets/purl/1607585"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://towardsdatascience.com/creating-synthetic-time-series-data-67223ff08e34"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Distinguishing time series</name>
    <datatype>Time series</datatype>
    <ability>Distinguish</ability>
    <datatoken>101</datatoken>
    <abilitytoken>114</abilitytoken>
    <description>Time series data have as one of their features the time, making this sequential data: the order matters. You can look at the general trends over time and distinguish data that does not match with the overall pattern or average. This capability is very useful for monitoring sensor data, as the time component is often of importance. An alternative to unsupervised distinguishing is to use supervised data, where the data is labeled as either being an outlier or not. In this case, the model will learn to predict if it is an outlier or not.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <output> <![CDATA[ Possible outlier 1 <br> Possible outlier 2]]></output>
    <techterm>Anomaly detection</techterm>
    <examples>
      <ex>
        <exname>Heart anomaly ECG</exname>
        <exdescription>One type of sensor data for which this capability can be used is ECG (heart) data. To manually monitor the ECG's of all patients is unfeasible, but by using this capability it is possible to train a model on what normal ECG's look like and then when there is an anomaly, the system will give an alarm to alert the nurses to check in on that patient.</exdescription>
        <eximage>assets/photos/heartanamolay.jpg</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://ieeexplore.ieee.org/document/7344872"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://www.kaggle.com/devavratatripathy/ecg-anomaly-detection-using-autoencoders"></diylink>
      </ex>
    </examples>
  </combi>
  <combi exist='no' NLP='yes'>
    <name></name>
    <datatype>Time series</datatype>
    <ability>Communicate</ability>
    <datatoken>101</datatoken>
    <abilitytoken>113</abilitytoken>
    <description>Time series data have as one of their features the time, making this sequential data: the order matters. You can look at the general trends over time and distinguish data that does not match with the overall pattern or average. This capability is very useful for monitoring sensor data, as the time component is often of importance.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <output> <![CDATA[ Possible outlier 1 <br> Possible outlier 2]]></output>
    <techterm>Anomaly detection</techterm>
    <examples>
      <ex>
        <exname></exname>
        <exdescription></exdescription>
        <eximage>assets/photos/</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href=""></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href=""></diylink>
      </ex>
    </examples>
  </combi>

  <combi>
    <name>Understanding time series</name>
    <datatype>Time series</datatype>
    <ability>Understand</ability>
    <datatoken>101</datatoken>
    <abilitytoken>122</abilitytoken>
    <description>Time series are data where the time, and hence the sequence, is an important feature. Examples are periodic measurements over time, sensor data and much more. To understand what is captured in the time series you can use time series categorization, as this will allow you to recognize the category to which the data belongs, and you can use that label to 'understand' it.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <output> <![CDATA[Labels or text]]></output>
    <techterm>Time series classification</techterm>
    <examples>
      <ex>
        <exname>Understanding sign language</exname>
        <exdescription>Based on the sensor data from sensors inside a glove, the movements of the hands can be tracked. By training a classification (categorize) model on this data, a model will be able to learn which movements fit with which gesture. Once this is known, the words can be for example written down or spoken out loud by a computer so that the gestures can be understood by others.</exdescription>
        <eximage>assets/photos/signlanguage.jfif</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://iopscience.iop.org/article/10.1088/1742-6596/1019/1/012017/pdf"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://cainvas.ai-tech.systems/use-cases/sign-language-sensor-app/"></diylink>
      </ex>
    </examples>
  </combi>
  <combi exist='no' NLP='yes'>
    <name></name>
    <datatype>Time series</datatype>
    <ability>Translate</ability>
    <datatoken>101</datatoken>
    <abilitytoken>117</abilitytoken>
    <description></description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <output> <![CDATA[ Label 5 (86%), Label 3 (14%) <br/> OR a predicted value (e.g. 453)]]></output>
    <techterm></techterm>
    <examples>
      <ex>
        <exname></exname>
        <exdescription></exdescription>
        <eximage>assets/photos/</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href=""></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href=""></diylink>
      </ex>
    </examples>
  </combi>
  <combi exist='no' NLP='yes'>
    <name></name>
    <datatype>Audio</datatype>
    <ability>Foresee</ability>
    <datatoken>97</datatoken>
    <abilitytoken>110</abilitytoken>
    <description></description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <output> <![CDATA[ Label 5 (86%), Label 3 (14%) <br/> OR a predicted value (e.g. 453)]]></output>
    <techterm></techterm>
    <examples>
      <ex>
        <exname></exname>
        <exdescription></exdescription>
        <eximage>assets/photos/</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href=""></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href=""></diylink>
      </ex>
    </examples>
  </combi>

  <combi>
    <name>Audio classification</name>
    <datatype>Audio</datatype>
    <ability>Categorize</ability>
    <datatoken>97</datatoken>
    <abilitytoken>109</abilitytoken>
    <description>Audio data is a special subset of time series data, and some similar techniques can be used. Using audio categorization, you can match certain segments of the audio with predefined classes (categories). For example you can recognize the pitch of a sound, match the bird song to a specific bird or recognize what someone is saying. For this last example you can either have a system that does not understand what is being said but just recognizes the sounds or you can use another ML ability (understand) to interpret what is being said.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <output> <![CDATA[ Label 5 (60%) <br/> Label 3 (23%) <br/> Label 2 (17%)]]></output>
    <techterm>Audio classification</techterm>
    <examples>
      <ex>
        <exname>Lego Duplo Stories</exname>
        <exdescription>With Lego Duplo stories, children can experience interactive stories. For this, it is using Alexa to tell the story. During this story, Alexa will give prompts to the child to build along with their Duplo and it will ask for input for the story. For example, it will ask what the theme is of the story (limited options) and the child can say for example 'Animal story'. Machine learning is here used to recognize what is being said and incorporates that into the story.</exdescription>
        <eximage>assets/photos/legoduplostories.jpg</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://www.amazon.com/LEGO-System-A-S-Stories/dp/B07C225J2T"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://www.tensorflow.org/tutorials/audio/simple_audio"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Audio identification</name>
    <datatype>Audio</datatype>
    <ability>Identify</ability>
    <datatoken>97</datatoken>
    <abilitytoken>111</abilitytoken>
    <description>Audio identification resembles audio categorization, but instead of matching the audio to a general category, you know link it to an individual. For example, recognizing who is speaking or which song is playing. With audio identification you are not able to understand what is being said, for this you will need for example audio understanding. </description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <output> <![CDATA[ Label 5 (86%), Label 3 (14%) <br/> OR a predicted value (e.g. 453)]]></output>
    <techterm>Audio classification, NLP</techterm>
    <examples>
      <ex>
        <exname>Voice match</exname>
        <exdescription>You can teach Google Assistant to recognize your voice with Voice Match. It will differentiate your voice from other speakers, so it can address you personally and give personal results. For example, when asking for your next meeting it will look at your personal calendar.</exdescription>
        <eximage>assets/photos/voicematch.jpg</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://support.google.com/chromecast/answer/9071681"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://github.com/jurgenarias/Portfolio/tree/master/Voice%20Classification"></diylink>
      </ex>
    </examples>
  </combi>

  <combi>
    <name>Audio clustering</name>
    <datatype>Audio</datatype>
    <ability>Cluster</ability>
    <datatoken>97</datatoken>
    <abilitytoken>112</abilitytoken>
    <description>Audio clustering will look for similarities between the audio fragments. Before it can do this, it is often needed to pre-process the audio by for example taking certain segments of the audio track. Next, it will find patterns in the data and cluster similar patterns together. However, it cannot tell you what is the commonality, human interpretation or labeled data is needed for this.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
        <output> <![CDATA[ Coordinates of the centroids <br> Which audio samples belong to which cluster]]></output>
    <techterm>Audio clustering</techterm>
    <examples>
      <ex>
        <exname>Bird sounds</exname>
        <exdescription>In this project, smart segments of bird sounds were taken and used without label as the input for a machine learning model. The model created some kind of fingerprints/ or images from the sounds and next group similar sounds closer together in a two-dimensional space. This eventually led to a map with bird sounds in which similar sounding birds are grouped closer together. After this process the labels of the sounds were added again to the data so you as user can see the name of the bird.</exdescription>
        <eximage>assets/photos/birdsounds.JPG</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://experiments.withgoogle.com/ai/bird-sounds/view/"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://towardsdatascience.com/clustering-music-to-create-your-personal-playlists-on-spotify-using-python-and-k-means-a39c4158589a"></diylink>
      </ex>
    </examples>
  </combi>

  <combi>
    <name>Audio recommendation</name>
    <datatype>Audio</datatype>
    <ability>Recommend</ability>
    <datatoken>97</datatoken>
    <abilitytoken>116</abilitytoken>
    <description>Audio recommendation can be done in two ways, it can recommend audio to the user that is similar to audio you have interacted with before/ liked. In this case it might use the metadata of e.g. music (genre, artist, etc.) or look for audio that sounds similar (see the example of audio clustering), this is content-based filtering. Another way is to recommend audio based on what similar users listened to and recommend those songs to you (collaborative filtering).</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
        <output> <![CDATA[ recommendation 1 <br> recommendation 2 <br> recommendation 3 ]]></output>
    <techterm>Recommender system audio, collaborative filtering audio, content based filtering audio</techterm>
    <examples>
      <ex>
        <exname>Spotify</exname>
        <exdescription>Spotify's Discover Weekly will use what you have listened to so far, as well as other information it has about you (your age, gender, location, etc.) and general information (season, hits, etc.) to recommend new music to you. For this they will use both filtering approaches as it will look at what similar users listened to and look for similar music.</exdescription>
        <eximage>assets/photos/spotify.png</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://www.spotify.com/"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://towardsdatascience.com/making-your-own-discover-weekly-f1ac7546fedb"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Audio generation</name>
    <datatype>Audio</datatype>
    <ability>Generate</ability>
    <datatoken>97</datatoken>
    <abilitytoken>115</abilitytoken>
    <description>Audio generation allows you to generate new audio from scratch. This could for example be new music, background noises or voices. For generating voices that speak 'human' language, you will also need another ML ability (communicate) to craft the messages that will be spoken.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
        <output> <![CDATA[New audio]]></output>
    <techterm>Audio synthesis, generative adversarial networks (GAN)</techterm>
    <examples>
      <ex>
        <exname>AIVA</exname>
        <exdescription>AIVA is a service that creates new music for different kind of purposes. You can select a genre for the new music piece or upload a track of music to with which it should match.</exdescription>
        <eximage>assets/photos/aiva.png</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://www.aiva.ai/"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://medium.com/neuronio/audio-generation-with-gans-428bc2de5a89"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Distinguish audio</name>
    <datatype>Audio</datatype>
    <ability>Distinguish</ability>
    <datatoken>97</datatoken>
    <abilitytoken>114</abilitytoken>
    <description>Audio data is a special subset of time series data, but focusses only on audio waves. Therefore, you can apply some of the same techniques to find general patterns and potential outliers. However, this might be more difficult if you are looking at speech, since you will need to process the audio as chunks of words or sentences. An alternative to unsupervised distinguishing is to use supervised data, where the data is labeled as either being an outlier or not. In this case, the model will learn to predict if it is an outlier or not.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <output> <![CDATA[ Possible outlier 1 <br> Possible outlier 2]]></output>
    <techterm>Audio anomaly detection</techterm>
    <examples exist='no'>
      <ex>
        <exname></exname>
        <exdescription></exdescription>
        <eximage>assets/photos/</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href=""></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href=""></diylink>
      </ex>
    </examples>
  </combi>

  <combi>
    <name>Speech/ audio communication</name>
    <datatype>Audio</datatype>
    <ability>Communicate</ability>
    <datatoken>97</datatoken>
    <abilitytoken>113</abilitytoken>
    <description>Besides communicating via written language, it is also possible to communicate through speech with e.g. your laptop, smart assistant or mobile phone. In the majority of cases they will first need to understand what you are asking (e.g. using audio or text understanding) and next they will reply to you via speech.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
        <output> <![CDATA[Speech]]></output>
    <techterm>NLP, text to speech</techterm>
    <examples>
      <ex>
        <exname>Digital humans</exname>
        <exdescription>A digital human can be seen as the upgraded function of a chatbot. Instead of communicating only via text, you will talk to a human face which shows expressions and which will talk back to you. For this it will of course needs to understand what you are saying (audio understanding), decide how to respond, and respond in understandable spoken human language. The digital human can - and are - used for, for example, customer support.</exdescription>
        <eximage>assets/photos/digitalhuman.png</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://digitalhumans.com/what-are-digital-humans/"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://github.com/human2b"></diylink>
      </ex>
    </examples>
  </combi>

  <combi>
    <name>Audio understanding</name>
    <datatype>Audio</datatype>
    <ability>Understand</ability>
    <datatoken>97</datatoken>
    <abilitytoken>122</abilitytoken>
    <description>Audio data are sound waves and with several techniques you can convert these audio waves into written text. For this, the audio is often first transformed from the time to frequency domain. Next, a model will listen to the sounds that are made and try to predict which sounds these are. These sounds are then translated into words. Finally, using an extra model which has knowledge of general human language, the sentences are build from these words. This extra model will help to create understandable sentences.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
        <output> <![CDATA[Text or actions]]></output>
    <techterm>Classification, NLP, Automatic Speech Recognition (ASR)</techterm>
    <examples>
      <ex>
        <exname>Smart assistants</exname>
        <exdescription> Smart assistants use this function to understand the commands you are giving and the questions you are asking. Based on this, it will often make an extra step to interpret what is being said and either execute that action (e.g. "Turn off the lights") or to respond (this will be covered by Audio communication).</exdescription>
        <eximage>assets/photos/smartassistants.jpg</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://en.wikipedia.org/wiki/Amazon_Alexa"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://towardsdatascience.com/how-to-build-your-own-ai-personal-assistant-using-python-f57247b4494b"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Audio translation</name>
    <datatype>Audio</datatype>
    <ability>Translate</ability>
    <datatoken>97</datatoken>
    <abilitytoken>117</abilitytoken>
    <description>Audio translation first uses audio understanding to convert the sound waves of your speech to text. Next, this text can be translated using text translation. To see more details, look at those combinations.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
        <output> <![CDATA[Translated text or speech]]></output>
    <techterm>NLP, Speech to text, machine translation</techterm>
    <examples>
      <ex>
        <exname>Live translate</exname>
        <exdescription>The live translate function from Microsoft makes it possible to speak in your own language and have it directly translated into different languages. This is done by first capturing what you are saying, converting that to words, translating those words and displaying this text to the other participants.</exdescription>
        <eximage>assets/photos/livetranslate.jpg</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://translator.microsoft.com/"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://www.geeksforgeeks.org/create-a-real-time-voice-translator-using-python/"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Predictions based on tabular data</name>
    <datatype>Table</datatype>
    <ability>Foresee</ability>
    <datatoken>99</datatoken>
    <abilitytoken>110</abilitytoken>
    <description>The data is stored in a table where each column holds information about one feature and each row represents one observation. Next to that, there is a column which contains the ground truth, which are the labels. These labels could be categorical (2 or more categories) or numerical (numbers on a scale). Based on all columns, except the one containing the labels, the ML model will try to predict what the ground truth label is.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <output> <![CDATA[ Label 5 (82%), Label 3 (18%) <br/> OR a predicted value (e.g. 23)]]></output>    <techterm>Regression, classification</techterm>
    <examples>
      <ex>
        <exname>Credit card fraud detection</exname>
        <exdescription> To detect credit card fraud, all transactions are stored in a table. Each row contains the information for one transaction, and each column contains the information about one feature. For this example, the features are not the original measurements but are combinations of multiple original measurements so that in the end we have fewer columns. Finally, one column contains the label if it was or was not a fraudulent transaction.
Based on all columns, except the one with the labels, the model will try to predict if it is credit card fraud.</exdescription>
        <eximage>assets/photos/creditcardfraud.jfif</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://www.mastercard.us/en-us/business/issuers/business-payments/fraud-prevention.html"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://towardsdatascience.com/credit-card-fraud-detection-using-machine-learning-python-5b098d4a8edc"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Categorize tabular data</name>
    <datatype>Table</datatype>
    <ability>Categorize</ability>
    <datatoken>99</datatoken>
    <abilitytoken>109</abilitytoken>
    <description>The data is stored in a table where each column holds information about one feature and each row represents one observation. Next to that, there is a column which contains the ground truth, which are the labels. When categorizing this data, the model will use the attributes from the table (all the columns) to predict for each instance to which category it belongs, e.g. what the ground truth label is.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <output> <![CDATA[ Label 5 (60%) <br/> Label 3 (23%) <br/> Label 2 (17%)]]></output>
    <techterm>Classification</techterm>
    <examples>
      <ex>
        <exname>Spotify</exname>
        <exdescription>Spotify, for example, stores the information about all the songs in a tabular format. The columns could be instruments, artists, length, etc., and one column contains the labels which in this case are the genres. The ML model will try to match each row to the correct genre based on the information in the other columns.</exdescription>
        <eximage>assets/photos/spotify.png</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://www.spotify.com/nl/"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://data-flair.training/blogs/python-project-music-genre-classification/"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Tabular data identification</name>
    <datatype>Table</datatype>
    <ability>Identify</ability>
    <datatoken>99</datatoken>
    <abilitytoken>111</abilitytoken>
    <description>Each row of the data contains information for one instance (observation). The columns of the data are the different attributes and one column contains the label, the ground truth. If you want to be able to categorize the identity of a specific individual/ item, this label should contain an identifier for that item, e.g. an ID number. The model will use the data from the other columns to learn how to identify the individuals/items. In order to be successful in this, the data should contain enough distinct information between the individuals.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <output> <![CDATA[ Label 5 (60%) <br/> Label 3 (23%) <br/> Label 2 (17%)]]></output>
    <techterm>Classification tabular data</techterm>
    <examples  exist='no'>
      <ex>
        <exname></exname>
        <exdescription></exdescription>
        <eximage>assets/photos/</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href=""></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href=""></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Clustering tabular data</name>
    <datatype>Table</datatype>
    <ability>Cluster</ability>
    <datatoken>99</datatoken>
    <abilitytoken>112</abilitytoken>
    <description>Each row of the data contains information for one instance (observation). The columns of the data are the different attributes that are measured, for example, how long you spend on one site and how many stars you gave it. Since clustering is an unsupervised learning technique, the table does not contain the ground truth labels. If there are labels present, they are only used as another attribute and not to check if the prediction was correct (this would be the ability categorize).</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
        <output> <![CDATA[ Coordinates of the centroids <br> Which instances belong to which cluster]]></output>
    <techterm>Clustering</techterm>
    <examples>
      <ex>
        <exname>Customer clustering</exname>
        <exdescription>To cluster customers, for example to send targeted discounts, all the information of the customers is saved in a table. Each row contains the information of one customer and the columns contain the features such as the products they buy, their usual time spend in the shop and if they buy products that are discounted. Based on these attributes, the clustering model will try to find patterns among these users and group customers together who buy for example similar products. There might be one group who always buys fast food and candy, and one group who mainly buys fresh products. However, in real life these groups will be more nuanced and overlap.</exdescription>
        <eximage>assets/photos/customersegment.png</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://www.segmentify.com/blog/customer-consumer-user-segmentation-examples"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://medium.com/codex/customer-segmentation-with-k-means-in-python-18336fb915be"></diylink>
      </ex>
    </examples>
  </combi>

  <combi>
    <name>Recommender system</name>
    <datatype>Table</datatype>
    <ability>Recommend</ability>
    <datatoken>99</datatoken>
    <abilitytoken>116</abilitytoken>
    <description>Recommender system will use tabular data to save all the characteristics of users and items (e.g. products). The table will have the information about one user or item stored in one row, and all the attributes/ characteristics are saved in the columns. These could be the scores you give; the number of interactions you have had; if you have bought an item or saved it as favorite. If the rows contain the items, it could be how much of it is bought and specifics about the type of product. Based on these tables, the recommender system can either recommend items that are very similar to the one you liked. For this, it will look in the table with items and see what other items have similar characteristics (content-based recommending). Or, it will look at what other users are similar to you and recommend the items they liked (collaborative filtering).</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
        <output> <![CDATA[ Recommendation 1 <br> Recommendation 2 <br> Recommendation 3 ]]></output>
    <techterm>Recommender system, collaborative filtering, content based filtering</techterm>
    <examples>
      <ex>
        <exname>Amazon</exname>
        <exdescription> Amazon uses both tables which contain the specifics of each product as well as the information about the users and their interactions with the product. So when you look at a product, it will recommend similar items based on the content of the item, for example recommend similar laptops, and it will look at what other similar users bought and also recommend a mouse, a laptop cover or a tablet.</exdescription>
        <eximage>assets/photos/amazon.png</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://www.amazon.com/"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://www.kaggle.com/saurav9786/recommender-system-using-amazon-reviews"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Synthetic tabular data</name>
    <datatype>Table</datatype>
    <ability>Generate</ability>
    <datatoken>99</datatoken>
    <abilitytoken>115</abilitytoken>
    <description>Sometimes you will need more data than you have in order to train for example a good model, or you have to take privacy issues into account and cannot share e.g. the patient data you have in tabular format. In both cases, you can generate new data that share the same characteristics as your original data. The new data will be created based on the original data.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
        <output> <![CDATA[New table]]></output>
    <techterm>Generative adversarial networks (GAN), synthetic tabular data</techterm>
    <examples>
      <ex>
        <exname>Tonic.ai</exname>
        <exdescription>TONIC.ai is a company whose specialty it is to generate new, fake, data based on the original data you provide. So you can have a dataset of tabular data and TONIC.ai will use this to generate more data that is similar but still different.</exdescription>
        <eximage>assets/photos/tonic.png</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://www.tonic.ai/"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://medium.com/analytics-vidhya/a-step-by-step-guide-to-generate-tabular-synthetic-dataset-with-gans-d55fc373c8db"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Distinguish tabular data</name>
    <datatype>Table</datatype>
    <ability>Distinguish</ability>
    <datatoken>99</datatoken>
    <abilitytoken>114</abilitytoken>
    <description>In tabular data, instances are stored as rows and features as columns. By analyzing the overall patterns and trends in the data, you can find instances which seem to be different. These might be outliers, due to for example a broken sensor, or other anomalies that you want to detect. An alternative to unsupervised distinguishing is to use supervised data, where the data is labeled as either being an outlier or not. In this case, the model will learn to predict if it is an outlier or not.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <output> <![CDATA[ Possible outlier 1 <br> Possible outlier 2]]></output>
    <techterm>Anomaly detection</techterm>
    <examples>
      <ex>
        <exname>Amazon monitron</exname>
        <exdescription>Amazon Monitron use this capability to monitor industrial equipment so that you can detect abnormal conditions and predict when maintenance is needed. For example, the temperature might suddenly be higher than the average, indicating a possible problem.</exdescription>
        <eximage>assets/photos/amazonmonitron.png</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://aws.amazon.com/monitron/"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://towardsdatascience.com/anomaly-detection-in-manufacturing-part-1-an-introduction-8c29f70fc68b"></diylink>
      </ex>
    </examples>
  </combi>
  <combi exist='no' NLP='yes'>
    <name></name>
    <datatype>Table</datatype>
    <ability>Communicate</ability>
    <datatoken>99</datatoken>
    <abilitytoken>113</abilitytoken>
    <description></description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
        <output> <![CDATA[Written text or speech]]></output>
    <techterm></techterm>
    <examples>
      <ex>
        <exname></exname>
        <exdescription></exdescription>
        <eximage>assets/photos/</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href=""></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href=""></diylink>
      </ex>
    </examples>
  </combi>
  <combi exist='no' NLP='yes'>
    <name></name>
    <datatype>Table</datatype>
    <ability>Understand</ability>
    <datatoken>99</datatoken>
    <abilitytoken>122</abilitytoken>
    <description></description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
        <output> <![CDATA[Written text or speech]]></output>
    <techterm></techterm>
    <examples>
      <ex>
        <exname></exname>
        <exdescription></exdescription>
        <eximage>assets/photos/</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href=""></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href=""></diylink>
      </ex>
    </examples>
  </combi>
  <combi exist='no' NLP='yes'>
    <name></name>
    <datatype>Table</datatype>
    <ability>Translate</ability>
    <datatoken>99</datatoken>
    <abilitytoken>117</abilitytoken>
    <description></description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
        <output> <![CDATA[Written text or speech]]></output>
    <techterm></techterm>
    <examples>
      <ex>
        <exname></exname>
        <exdescription></exdescription>
        <eximage>assets/photos/</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href=""></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href=""></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Video prediction</name>
    <datatype>Video</datatype>
    <ability>Foresee</ability>
    <datatoken>102</datatoken>
    <abilitytoken>110</abilitytoken>
    <description>Since videos can be seen as time series of images with audio, you could also use some of those techniques. For example, you could predict what the next frame of a video will be or what will be said next.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <output> <![CDATA[ Possible next frame 1 (92%) <br> Possible next frame 2 (8%) <br> OR a predicted value (e.g. 67)]]></output>
    <techterm>Video prediction</techterm>
    <examples>
      <ex>
        <exname>Self-driving cars</exname>
        <exdescription>Video prediction is nowadays being used for self-driving cars. Here, a camera will film the environment of the car and detect where other vehicles or people are. Using the ability foresee, they are trying to predict the next frame, which in this situation will inform you where other vehicles or people are going to. This can then be used to anticipate and redirect the trajectory of the car if needed.</exdescription>
        <eximage>assets/photos/selfdrivingcars.jpg</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://www.youtube.com/watch?v=yEtH23rKY8Q"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://towardsdatascience.com/deeppicar-part-1-102e03c83f2c"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Video classification</name>
    <datatype>Video</datatype>
    <ability>Categorize</ability>
    <datatoken>102</datatoken>
    <abilitytoken>109</abilitytoken>
    <description>Videos can be matched with categories based on their entire content. For example, a video can be recognized as a sport video or a cooking video. In this case, there is one label for the entire video.
Another option is to see each frame of the video as an image and have labels for each frame. In this case, machine learning models for images could also be used for recognizing categories in videos (for example, recognizing cats and dogs in videos). In order to recognize multiple objects in one frame, you need, just as with images, a bounding box around each object.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
        <output> <![CDATA[ Label 5 (60%) <br/> Label 3 (23%) <br/> Label 2 (17%)]]></output>
    <techterm>Video classification</techterm>
    <examples>
      <ex>
        <exname>Zenia</exname>
        <exdescription>Zenia is an app which provide yoga classes and uses pose recognition to give personalized feedback. For this it will analyse each frame of the image, recognize the locations of the joints and next use this to see if you are doing the right pose and if you do the pose right.</exdescription>
        <eximage>assets/photos/zenia.png</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://zenia.app/"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://thecodingtrain.com/learning/ml5/7.1-posenet.html"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Video clustering</name>
    <datatype>Video</datatype>
    <ability>Cluster</ability>
    <datatoken>102</datatoken>
    <abilitytoken>112</abilitytoken>
    <description>Videos can be clustered in their entirety by looking for similar attributes of the video itself or its metadata. Next to that, it can also cluster frames within the video and this will resemble image clustering.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
        <output> <![CDATA[ Coordinates of the centroids <br> Which videos belong to which cluster]]></output>
    <techterm>Video clustering</techterm>
    <examples>
      <ex>
        <exname>Video summarization</exname>
        <exdescription>In this paper, they describe a technique to use video clustering to create a summary of the video. For this they split the video in short segments, use the first frame of each segment and use these for the clustering. If the frames are not too much alike (they do not belong to the same cluster), their segment will be used for the summary.</exdescription>
        <eximage>assets/photos/videocluster.png</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://www.researchgate.net/publication/266032463_Video_Summarization_Using_Clustering"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href=""></diylink>
      </ex>
    </examples>
  </combi>

  <combi>
    <name>Video recommendation</name>
    <datatype>Video</datatype>
    <ability>Recommend</ability>
    <datatoken>102</datatoken>
    <abilitytoken>116</abilitytoken>
    <description>Video recommendation can be based on the content and the metadata of the video. In this case, videos that share the same content and/or metadata will be recommended to you (content-based filtering). For example, when you have watched a lot of videos that explain Machine Learning it will recommend more videos on this topic. Or it will use your past behavior to find users with a similar bevhavior, e.g. you have watched or liked the same videos (collaborative-filtering). Then it will look at what these users also liked and recommend these items to you. For example, it will recommend programming videos, since similar users watched both machine learning and programming videos.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
        <output> <![CDATA[ recommendation 1 <br> recommendation 2 <br> recommendation 3]]></output>
    <techterm>Recommender system video, collaborative filtering video, content based filtering video</techterm>
    <examples>
      <ex>
        <exname>Youtube</exname>
        <exdescription>YouTube is doing exactly what is described above, it will use both approaches to recommend new videos to you as well recommend the overall popular videos.</exdescription>
        <eximage>assets/photos/youtube.jpg</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://www.youtube.com/"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://www.datacamp.com/community/tutorials/recommender-systems-python"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Video generation</name>
    <datatype>Video</datatype>
    <ability>Generate</ability>
    <datatoken>102</datatoken>
    <abilitytoken>115</abilitytoken>
    <description>With video generation you can either create videos from scratch, based on a few parameters, or base it on already existing videos. In this last scenario, it can be very hard to distinguish real from fake, and these videos are generally known as deep fakes.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
        <output> <![CDATA[New video]]></output>
    <techterm>Video generative adversarial network (GAN)</techterm>
    <examples>
      <ex>
        <exname>Deep fakes</exname>
        <exdescription>Videos that are adapted from an original video where for example faces are swapped or where the person is saying something else compared to the original video.</exdescription>
        <eximage>assets/photos/deepfake.jpg</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://edition.cnn.com/interactive/2019/01/business/pentagons-race-against-deepfakes/"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://github.com/xaliceli/video-GAN"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Video identification</name>
    <datatype>Video</datatype>
    <ability>Identify</ability>
    <datatoken>102</datatoken>
    <abilitytoken>111</abilitytoken>
    <description>Videos can be seen as a series of images, and some similar techniques can be used. For example, it is possible to recognize individuals or individual objects in the frames of the video. In this case the frames are seen as images and the same type of ML model can be used.
Next to that, videos can also contain audio and for this you can audio identification to identify who is speaking.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
  <output> <![CDATA[ Label 5 (60%) <br/> Label 3 (23%) <br/> Label 2 (17%)]]></output>
    <techterm>Facial recognition</techterm>
    <examples>
      <ex>
        <exname>Face ID</exname>
        <exdescription>After an initial training phase where the ML model is learned to recognize your face specifically, it is possible to use this as a unique identifier to for example unlock your phone or verify your online payment.</exdescription>
        <eximage>assets/photos/faceid.jpg</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://support.apple.com/en-us/HT208108"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://github.com/ageitgey/face_recognition"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Distinguish video data</name>
    <datatype>Video</datatype>
    <ability>Distinguish</ability>
    <datatoken>102</datatoken>
    <abilitytoken>114</abilitytoken>
    <description>Videos can sometimes be treated as a combination of a time series of images and audio. This makes it possible to use some of the same techniques as for images or audio. So you can find some general patterns or trends and potential outliers. However, when viewing the frames as separate images, you lose the sequential aspect which might be of importance for finding patterns. An alternative to unsupervised distinguishing is to use supervised data, where the data is labeled as either being an outlier or not. In this case, the model will learn to predict if it is an outlier or not.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <output> <![CDATA[ Possible outlier 1 <br> Possible outlier 2]]></output>
    <techterm>Anomaly detection</techterm>
    <examples exist='no'>
      <ex>
        <exname></exname>
        <exdescription></exdescription>
        <eximage>assets/photos/</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href=""></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href=""></diylink>
      </ex>
    </examples>
  </combi>
  <combi exist='no' NLP='yes'>
    <name></name>
    <datatype>Video</datatype>
    <ability>Communicate</ability>
    <datatoken>102</datatoken>
    <abilitytoken>113</abilitytoken>
    <description></description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
        <output> <![CDATA[Written text or speech]]></output>
    <techterm></techterm>
    <examples>
      <ex>
        <exname></exname>
        <exdescription></exdescription>
        <eximage>assets/photos/</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href=""></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href=""></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Video understanding</name>
    <datatype>Video</datatype>
    <ability>Understand</ability>
    <datatoken>102</datatoken>
    <abilitytoken>122</abilitytoken>
    <description>To understanding what is seen and heard in a video, you can combine image and audio understanding. The first will look at what can be seen in the video and convert this to text. Next to that, you can also create automatic captions by converting the audio to text.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
        <output> <![CDATA[Written text]]></output>
    <techterm>NLP, speech to text, image to text</techterm>
    <examples>
      <ex>
        <exname>Automatic subtitles</exname>
        <exdescription> YouTube videos can automatically create captions for videos. To do this, they use speech recognition (audio understanding) and display the text at the right time in the video. </exdescription>
        <eximage>assets/photos/youtube.jpg</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://support.google.com/youtube/answer/6373554"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://pypi.org/project/autosub/"></diylink>
      </ex>
    </examples>
  </combi>
  <combi>
    <name>Video translation</name>
    <datatype>Video</datatype>
    <ability>Translate</ability>
    <datatoken>102</datatoken>
    <abilitytoken>117</abilitytoken>
    <description>Videos can be seen as a time series of images with audio. If you look at this way, some of the possible models for images can also be used for video by applying it to every frame in the video. This is also possible for transferring the style from one image or video to another video.</description>
    <capabilities>
      <c1 value=""></c1>
      <c2 value=""></c2>
      <c3 value=""></c3>
    </capabilities>
    <limitations>
      <l1 value=""></l1>
      <l2 value=""></l2>
      <l3 value=""></l3>
    </limitations>
    <output> <![CDATA[Video in a different style]]></output>
    <techterm>Neural style transfer</techterm>
    <examples>
      <ex>
        <exname>Video style transfer</exname>
        <exdescription>With video style transfer you can give your video for example an animated look. You select a reference style and next this style will be transferred to your video frame for frame. To make sure that the frames still match, this is done sequentially so that the new frame can be connected to the previous frame,</exdescription>
        <eximage>assets/photos/videostyletransfer.png</eximage>
        <exlink xlink:type="simple" xlink:show="new" xlink:href="https://rcorin.medium.com/video-style-transfer-with-art-app-d0d73abe7963"></exlink>
        <diylink xlink:type="simple" xlink:show="new" xlink:href="https://towardsdatascience.com/neural-style-transfer-on-real-time-video-with-full-implementable-code-ac2dbc0e9822"></diylink>
      </ex>
    </examples>
  </combi>
</combinations>
